{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Participant Enrollment Overview (v2.1)\n",
    "\n",
    "First automated version with 2018 updates.\n",
    "Scott Sutherland (ssutherland@vibrenthealth.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "print('Setting everything up...')\n",
    "#######################################\n",
    "\n",
    "import sys\n",
    "# Add the libraries folder path to the sys.path list\n",
    "sys.path.append('../lib/')\n",
    "\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "from progress import Progress\n",
    "from ubr import UBRRaceEthnicity, UBRAge, UBRSex, UBRSexualAndGenderMinorities, UBRIncome\n",
    "from ubr import UBREducation, LoadUBRZipCodes, UBRGeography, UBRAccessToCare, UBRDisability\n",
    "\n",
    "\n",
    "d = '2019-01-30'\n",
    "d_prev = '2019-01-29'\n",
    "w_prev = '2019-01-23'\n",
    "\n",
    "print('done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "print('Getting the data from the database...')\n",
    "######################################\n",
    "connect_options = {\n",
    "    'user': config.CLOUDSQL_PROD_USER,\n",
    "    'password': config.CLOUDSQL_PROD_PASSWORD,\n",
    "    'host': '127.0.0.1',\n",
    "    'port': '3307',\n",
    "    'database': 'rdr',\n",
    "    'raise_on_warnings': True,\n",
    "}\n",
    "\n",
    "try:\n",
    "      cnx = mysql.connector.connect(**connect_options)\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "        print(\"Something is wrong with your user name or password\")\n",
    "    elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "        print(\"Database does not exist\")\n",
    "    else:\n",
    "        print(err)\n",
    "else:\n",
    "\n",
    "    #query = (\"DESCRIBE rdr.participant_view;\")\n",
    "    query = ('''   \n",
    "\n",
    "    SELECT \n",
    "    participant_id, \n",
    "    hpo, \n",
    "    s.google_group pm_site, \n",
    "    enrollment_status, \n",
    "    withdrawal_status, \n",
    "    sex, \n",
    "    gender, \n",
    "    sexual_orientation, \n",
    "    (CASE /*######## ToDo: make sure age_years is based on age at registration ########*/ \n",
    "        WHEN age_years IS NULL THEN ''  \n",
    "        WHEN age_years < 18 THEN '0-17'  \n",
    "        WHEN age_years < 26 THEN '18-25'  \n",
    "        WHEN age_years < 36 THEN '26-35'  \n",
    "        WHEN age_years < 46 THEN '36-45'  \n",
    "        WHEN age_years < 56 THEN '46-55'  \n",
    "        WHEN age_years < 66 THEN '56-65'  \n",
    "        WHEN age_years < 76 THEN '66-75'  \n",
    "        WHEN age_years < 86 THEN '76-85'  \n",
    "        ELSE '86+'  END) \n",
    "    age_bucket, \n",
    "    age_years, /*######## ToDo: get rid of this eventually in favor of age_bucket ######## */\n",
    "    race, \n",
    "    race_codes, \n",
    "    hispanic, \n",
    "    income, \n",
    "    education, \n",
    "    pv.state \n",
    "    state, \n",
    "    pv.zip_code \n",
    "    zip_code, \n",
    "    LEFT(pv.zip_code, 3) AS zip_code_3_digit, \n",
    "    DATE(sign_up_time) signup_date,  \n",
    "    consent_for_study_enrollment, \n",
    "    DATE(consent_for_study_enrollment_time) consent_date,  \n",
    "    consent_for_electronic_health_records, \n",
    "    DATE(consent_for_electronic_health_records_time) ehr_date,  \n",
    "    questionnaire_on_the_basics, \n",
    "    DATE(questionnaire_on_the_basics_time) ppi_thebasics_date,  \n",
    "    questionnaire_on_overall_health, \n",
    "    DATE(questionnaire_on_overall_health_time) ppi_overallhealth_date,  \n",
    "    questionnaire_on_lifestyle, \n",
    "    DATE(questionnaire_on_lifestyle_time) ppi_lifestyle_date,  \n",
    "    questionnaire_on_healthcare_access, \n",
    "    DATE(questionnaire_on_healthcare_access_time) ppi_hc_access_date,  \n",
    "    questionnaire_on_medical_history, \n",
    "    DATE(questionnaire_on_medical_history_time) ppi_med_history_date,  \n",
    "    questionnaire_on_family_health, \n",
    "    DATE(questionnaire_on_family_health_time) ppi_family_health_date,  \n",
    "    questionnaire_on_medications, \n",
    "    DATE(questionnaire_on_medications_time) ppi_medications_date,  \n",
    "    physical_measurements_status, \n",
    "    DATE(physical_measurements_finalized_time) pm_date,  \n",
    "    DATE(sample_status_1ed04_time) sample_date, \n",
    "    DATE(sample_status_1sal_time) 1sal_date,  \n",
    "    sample_order_status_1sst8, \n",
    "    sample_order_status_1pst8, \n",
    "    sample_order_status_1hep4, \n",
    "    sample_order_status_1ed04, \n",
    "    sample_order_status_1ed10, \n",
    "    sample_order_status_2ed10, \n",
    "    sample_order_status_1cfd9, \n",
    "    sample_order_status_1pxr2, \n",
    "    sample_order_status_1sal, \n",
    "    sample_order_status_1sal2, \n",
    "    sample_order_status_1ur10,  \n",
    "    sample_status_1sst8, \n",
    "    sample_status_1pst8, \n",
    "    sample_status_1hep4, \n",
    "    sample_status_1ed04, \n",
    "    sample_status_1ed10, \n",
    "    sample_status_2ed10, \n",
    "    sample_status_1cfd9, \n",
    "    sample_status_1pxr2, \n",
    "    sample_status_1sal, \n",
    "    sample_status_1sal2, \n",
    "    sample_status_1ur10, \n",
    "    samples_to_isolate_dna,  \n",
    "\n",
    "    DATE(sign_up_time) as registered_date,\n",
    "    DATE(\n",
    "        CASE WHEN enrollment_status >=2 THEN\n",
    "            GREATEST( sign_up_time\n",
    "                , COALESCE(consent_for_study_enrollment_time, '1000-01-01')\n",
    "                , COALESCE(consent_for_electronic_health_records_time, '1000-01-01')\n",
    "                )\n",
    "            ELSE NULL END\n",
    "        ) member_date,\n",
    "\n",
    "    DATE(\n",
    "        CASE WHEN enrollment_status = 3 THEN /*######## ToDo: Make this not rely on ES ########*/\n",
    "            GREATEST( sign_up_time\n",
    "                , COALESCE(consent_for_study_enrollment_time, '1000-01-01')\n",
    "                , COALESCE(consent_for_electronic_health_records_time, '1000-01-01')\n",
    "                , COALESCE(questionnaire_on_the_basics_time, '1000-01-01')\n",
    "                , COALESCE(questionnaire_on_overall_health_time, '1000-01-01')\n",
    "                , COALESCE(questionnaire_on_lifestyle_time, '1000-01-01')\n",
    "                , COALESCE(physical_measurements_finalized_time, '1000-01-01')\n",
    "                , CASE WHEN\n",
    "                         LEAST(\n",
    "                            COALESCE(sample_status_1ed04_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_1sal_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_1sal2_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_1ed10_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_2ed10_time, '3000-01-01')\n",
    "                            ) = '3000-01-01' THEN NULL\n",
    "                    ELSE LEAST(\n",
    "                            COALESCE(sample_status_1ed04_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_1sal_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_1sal2_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_1ed10_time, '3000-01-01')\n",
    "                            , COALESCE(sample_status_2ed10_time, '3000-01-01')\n",
    "                            )    \n",
    "                    END\n",
    "                )\n",
    "            ELSE NULL END\n",
    "        ) fp_date\n",
    "\n",
    "\n",
    "    FROM rdr.participant_view pv \n",
    "    LEFT OUTER JOIN rdr.site s on s.site_id = pv.physical_measurements_finalized_site_id \n",
    "    WHERE \n",
    "        withdrawal_status = 1 \n",
    "    ORDER BY sign_up_time ;\n",
    "\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    #Now execute the query and put it it into a dataframe\n",
    "    myData = pd.read_sql(query, \n",
    "                         con=cnx,  \n",
    "                         parse_dates=['signup_date', 'consent_date', 'ehr_date', 'ppi_thebasics_date', \n",
    "                                      'ppi_overallhealth_date', 'ppi_lifestyle_date', 'ppi_med_history_date', \n",
    "                                      'ppi_hc_access_date', 'ppi_family_health_date', 'ppi_medications_date', \n",
    "                                      'pm_date', 'sample_date', '1sal_date', \n",
    "                                      'registered_date', 'member_date', 'fp_date'])\n",
    "    \n",
    "    #Close the connection\n",
    "    cnx.close()\n",
    "    \n",
    "    print(myData.shape[0], 'records received.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "print('Adding extra columns for searching, sorting, and filtering...')\n",
    "#######################################\n",
    "\n",
    "## Add the needed columns to our dataframe\n",
    "\n",
    "#Add columns for the UBR fields and columns needed for the PEO\n",
    "myData.insert(loc=0, column = 'UBR', value = 0)\n",
    "myData.insert(loc=0, column = 'UBRScore', value = 0)\n",
    "myData.insert(loc=0, column = 'UBRCategory', value = '')\n",
    "myData.insert(loc=0, column = 'UBRMultipleCategories', value = '')\n",
    "myData.insert(loc=0, column = 'UBR1_RaceEthnicity', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR2_Age', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR3_Sex', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR4_SexualAndGenderMinorities', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR5_Income', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR6_Education', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR7_Geography', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR8_AccessToCare', value = 0)\n",
    "myData.insert(loc=0, column = 'UBR9_Disability', value = 0)\n",
    "myData.insert(loc=0, column = 'SimplifiedRace', value = '')\n",
    "myData.insert(loc=0, column = 'Ancestry', value = '')\n",
    "myData.insert(loc=0, column = 'in_includelist', value = 0)\n",
    "myData.insert(loc=0, column = 'in_excludelist', value = 0)\n",
    "myData.insert(loc=0, column = 'in_exclude_ghosts_list', value = 0)\n",
    "\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "print(\"Joining in additional columns we didn't get in the primary SQL...\")\n",
    "#######################################\n",
    "\n",
    "# Do this: \n",
    "#     load dataframe that includes timestamp called \"response_time\"\n",
    "#     Sort it: dataToJoin.sort_values(\"response_time\", inplace = True)\n",
    "#     Drop dups: dataToJoin.drop_duplicates(subset =\"participant_id\", keep = \"first\", inplace = True)\n",
    "#     Then do the join, only keeeping the colums we want (not timestamp)\n",
    "\n",
    "print('myData dimensions: {}'.format(myData.shape))\n",
    "\n",
    "\n",
    "#########\n",
    "print('• Loading DV EHR Intent data')\n",
    "#########\n",
    "connect_options = {\n",
    "    'user': config.CLOUDSQL_PROD_USER,\n",
    "    'password': config.CLOUDSQL_PROD_PASSWORD,\n",
    "    'host': '127.0.0.1',\n",
    "    'port': '3307',\n",
    "    'database': 'rdr',\n",
    "    'raise_on_warnings': True,\n",
    "}\n",
    "\n",
    "try:\n",
    "      cnx = mysql.connector.connect(**connect_options)\n",
    "except mysql.connector.Error as err:\n",
    "    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "        print(\"Something is wrong with your user name or password\")\n",
    "    elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "        print(\"Database does not exist\")\n",
    "    else:\n",
    "        print(err)\n",
    "else:\n",
    "    query = ('''   \n",
    "    SELECT\n",
    "        p.participant_id,\n",
    "        ac.value AS dv_ehr_intent_status,\n",
    "        qr.created AS dv_ehr_intent_time\n",
    "    FROM\n",
    "        rdr.participant p\n",
    "        INNER JOIN rdr.questionnaire_response qr ON p.participant_id = qr.participant_id\n",
    "        INNER JOIN rdr.questionnaire_response_answer qra\n",
    "           ON qra.questionnaire_response_id = qr.questionnaire_response_id\n",
    "        INNER JOIN rdr.questionnaire_question qq ON qra.question_id = qq.questionnaire_question_id\n",
    "        INNER JOIN rdr.questionnaire q ON qq.questionnaire_id = q.questionnaire_id\n",
    "        INNER JOIN rdr.code qc ON qq.code_id = qc.code_id\n",
    "        INNER JOIN rdr.participant_summary ps ON p.participant_id = ps.participant_id\n",
    "        LEFT OUTER JOIN rdr.hpo ON p.hpo_id = hpo.hpo_id\n",
    "        LEFT OUTER JOIN rdr.code ac ON qra.value_code_id = ac.code_id\n",
    "    WHERE \n",
    "        p.withdrawal_status < 2 AND /* NOT_WITHDRAWN*/\n",
    "        qc.value = 'DVEHRSharing_AreYouInterested' AND \n",
    "        (ps.email IS NULL OR ps.email NOT LIKE '%@example.com') AND\n",
    "        (hpo.name IS NULL OR hpo.name != 'TEST'); \n",
    "    ''')\n",
    "    \n",
    "    #Now execute the query and put it it into a dataframe\n",
    "    dataToJoin = pd.read_sql(query, con=cnx, parse_dates=['dv_ehr_intent_time'])\n",
    "    \n",
    "    #Close the connection\n",
    "    cnx.close()\n",
    "\n",
    "print('data-file dimensions: {}'.format(dataToJoin.shape))\n",
    "\n",
    "## Old version, reading from data file\n",
    "# inFilePath = '2018-12-28_DV_EHR_intent_data.csv'    \n",
    "# dataToJoin = pd.read_csv(inFilePath, parse_dates=['dv_ehr_intent_time'])\n",
    "dataToJoin.sort_values(\"dv_ehr_intent_time\", inplace = True)    #Sort it first\n",
    "dataToJoin.drop_duplicates(subset =\"participant_id\", keep = \"last\", inplace = True)   #Keep the final value for a given PID if there are duplicates \n",
    "myData = pd.merge(myData, dataToJoin, how='left', on='participant_id', validate='one_to_one') #merge, checking to make sure join is 1:1 (that is, no dups in file being merged)\n",
    "print('result dimensions: {}'.format(myData.shape))\n",
    "\n",
    "# print()\n",
    "\n",
    "#########\n",
    "print('• Loading DV Market data')\n",
    "#########\n",
    "\n",
    "inFilePath = '../data/2018-12-13-markets.csv'    \n",
    "dataToJoin = pd.read_csv(inFilePath, dtype={'zip_3_digit': str})\n",
    "print('data-file dimensions: {}'.format(dataToJoin.shape))\n",
    "#dataToJoin.sort_values(\"dv_ehr_intent_time\", inplace = True)    #Sort it first\n",
    "#dataToJoin.drop_duplicates(subset =\"participant_id\", keep = \"last\", inplace = True)   #Keep the final value for a given PID if there are duplicates \n",
    "myData = pd.merge(myData, dataToJoin, how='left', left_on='zip_code_3_digit', right_on='zip_3_digit', validate='many_to_one') #merge, checking to make sure join is 1:1 (that is, no dups in file being merged)\n",
    "print('result dimensions: {}'.format(myData.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "print('Filling the columns with calculated values...')\n",
    "#######################################\n",
    "\n",
    "#Set this value to setermine whether women are counted in the UBR categories or not\n",
    "womenAreUBR = False\n",
    "\n",
    "#Set the file path to the UBR zip codes, and uncommnt the line below\n",
    "zipcodesFilePath = '../data/Rural classified zip codes US 1 5 18.csv'\n",
    "zips = pd.read_csv(zipcodesFilePath)\n",
    "\n",
    "#get the total number of participants for use later\n",
    "participantCount = myData.shape[0]\n",
    "\n",
    "#set up progress bar\n",
    "p = Progress(participantCount,10)\n",
    "\n",
    "#Iterate through the participants and calculate fields\n",
    "sexCount = 0\n",
    "for index, row in myData.iterrows():\n",
    "    #ToDo: Change this next line when we have access to age at consent.\n",
    "    myData.at[index,'UBR1_RaceEthnicity'] = UBRRaceEthnicity(row['race_codes'], row['hispanic']) #set UBR1_RaceEthnicity field to value returned from UBRRaceEthnicity() \n",
    "    myData.at[index,'UBR2_Age'] = UBRAge(row['age_years']) #set UBR2_Age field to value returned from UBRAge() \n",
    "    myData.at[index,'UBR3_Sex'] = UBRSex(row['sex'], womenAreUBR) #set UBR3_Sex field to value returned from UBRSex() \n",
    "    myData.at[index,'UBR4_SexualAndGenderMinorities'] = UBRSexualAndGenderMinorities(row['sexual_orientation'], row['gender'], row['sex'], womenAreUBR) #set UBR4_SexualAndGenderMinorities field to value returned from UBRSexualAndGenderMinorities() \n",
    "    myData.at[index,'UBR5_Income'] = UBRIncome(row['income']) #set UBR5_Income field to value returned from UBRIncome() \n",
    "    myData.at[index,'UBR6_Education'] = UBREducation(row['education']) #set UBR6_Education field to value returned from UBREducation() \n",
    "    myData.at[index,'UBR7_Geography'] = UBRGeography(row['zip_code'], zips.ZIP.values) #set UBR7_Geography field to value returned from UBRGeography() \n",
    "    #myData.at[index,'UBR8_AccessToCare'] = UBRAccessToCare() #set UBR7_Geography field to value returned from UBRGeography() \n",
    "    #myData.at[index,'UBR9_Disability'] = UBRDisability() #set UBR7_Geography field to value returned from UBRGeography() \n",
    "\n",
    "    \n",
    "    \n",
    "    #set UBRScore to the sum of the UBR individual scores\n",
    "    myData.at[index,'UBRScore'] = (myData['UBR1_RaceEthnicity'][index] + \n",
    "                                          myData['UBR2_Age'][index] + \n",
    "                                          myData['UBR3_Sex'][index] +\n",
    "                                          myData['UBR4_SexualAndGenderMinorities'][index] + \n",
    "                                          myData['UBR5_Income'][index] + \n",
    "                                          myData['UBR6_Education'][index] + \n",
    "                                          myData['UBR7_Geography'][index] + \n",
    "                                          myData['UBR8_AccessToCare'][index] + \n",
    "                                          myData['UBR9_Disability'][index]\n",
    "                                         ) \n",
    "    \n",
    "    #Set UBR to 1 if _one_or_more_ UBR categories are set. Otherwise set it to 0\n",
    "    if myData['UBRScore'][index] > 0:\n",
    "        myData.at[index,'UBR'] = 1\n",
    "    else:\n",
    "        myData.at[index,'UBR'] = 0\n",
    "        \n",
    "    #Set UBRCategory to the category set if only qualified in 1 UBR category\n",
    "    # set to \"Multiple Categories\" if more than one category is set. \n",
    "    # set to \"Not UBR\" if no categories are set.\n",
    "    \n",
    "    ubrCategories = [\n",
    "        'UBR9_Disability',\n",
    "        'UBR8_AccessToCare',\n",
    "        'UBR7_Geography',\n",
    "        'UBR6_Education',\n",
    "        'UBR5_Income',\n",
    "        'UBR4_SexualAndGenderMinorities',\n",
    "        'UBR3_Sex',\n",
    "        'UBR2_Age',\n",
    "        'UBR1_RaceEthnicity']\n",
    "    \n",
    "    if myData['UBRScore'][index] == 0:\n",
    "        myData.at[index,'UBRCategory'] = \"Not UBR\"\n",
    "    elif myData['UBRScore'][index] > 1:\n",
    "        myData.at[index,'UBRCategory'] = \"Multiple_Categories\"\n",
    "        for category in ubrCategories:\n",
    "            if myData[category][index] == 1:\n",
    "                myData.at[index,'UBRMultipleCategories'] += (category + \",\")\n",
    "    else:\n",
    "        for category in ubrCategories:\n",
    "            if myData[category][index] == 1:\n",
    "                myData.at[index,'UBRCategory'] = category\n",
    "        \n",
    "   \n",
    "\n",
    "    #Increase the status bar\n",
    "    p.increase()\n",
    "    \n",
    "#print(c_1)\n",
    "print()\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "print('Applying the inclusion and exclusion lists...')\n",
    "#######################################\n",
    "\n",
    "###\n",
    "# Get the includelist PIDs and excludelist PIDs\n",
    "###\n",
    "filePath = 'All_known_pids.csv'\n",
    "#filePath = '2018-09-17_PIDs_for_Holly.csv'\n",
    "includelist = pd.read_csv(filePath)\n",
    "\n",
    "filePath = '../data/2018-08-20-non_participant_PIDs.txt'\n",
    "excludelist = pd.read_csv(filePath)\n",
    "\n",
    "filePath = '../data/ghost_participants_2019-01-31.csv'\n",
    "exclude_ghosts_list = pd.read_csv(filePath)\n",
    "\n",
    "\n",
    "####\n",
    "# Fill the in_includelist, in_excludelist columns\n",
    "####\n",
    "\n",
    "for index, row in myData.iterrows():\n",
    "    #set the includelist column value\n",
    "    if int(row['participant_id']) in includelist.AOU_PID.values:\n",
    "        myData.at[index,'in_includelist'] = 1\n",
    "    else:\n",
    "        myData.at[index,'in_includelist'] = 0\n",
    "        \n",
    "    #set the in_excludelist column value\n",
    "    if int(row['participant_id']) in excludelist.participant_id.values:\n",
    "        myData.at[index,'in_excludelist'] = 1\n",
    "    else:\n",
    "        myData.at[index,'in_excludelist'] = 0\n",
    "        \n",
    "    #set the in_exclude_ghosts_list column value\n",
    "    if int(row['participant_id']) in exclude_ghosts_list.participant_id.values:\n",
    "        myData.at[index,'in_exclude_ghosts_list'] = 1\n",
    "    else:\n",
    "        myData.at[index,'in_exclude_ghosts_list'] = 0\n",
    "        \n",
    "####\n",
    "# See how many we found\n",
    "####\n",
    "print(myData.shape[0], 'participant records received.')\n",
    "print(includelist.shape[0], 'includelist records received.')\n",
    "print(excludelist.shape[0], 'excludelist records received.')\n",
    "\n",
    "print('Total ghosts found:', myData[(myData['in_includelist'] == 0)].shape[0])\n",
    "\n",
    "print('Total participants in includelist:', myData[(myData['in_includelist'] == 1)].shape[0])\n",
    "print('Total participants NOT in excludelist:', myData[(myData['in_excludelist'] == 0)].shape[0])\n",
    "print('Total participants IN includelist and NOT in excludelist:', myData[\n",
    "        (myData['in_includelist'] == 1) & \n",
    "        (myData['in_excludelist'] == 0)\n",
    "    ].shape[0])\n",
    "#print('Total Ppts with EHR:', myData[(filtered_data['has_EHR'] == 1)].shape[0])\n",
    "\n",
    "####\n",
    "#Finally, apply the filters to get the new, filtered dataset\n",
    "####\n",
    "how = 'use_ghost_list' # should = 'use_pid_master_list' only if you want to read a whole master list of PIDs\n",
    "\n",
    "if how == 'use_pid_master_list':\n",
    "    print('removing any ghosts not in master list')\n",
    "    filtered_data = myData.loc[\n",
    "        (myData['in_includelist'] == 1) & \n",
    "        (myData['in_excludelist'] == 0) ]\n",
    "else:  # 'use_ghosts'\n",
    "    print('removing specific ghosts in list')\n",
    "    filtered_data = myData.loc[\n",
    "        (myData['in_exclude_ghosts_list'] == 0) & \n",
    "        (myData['in_excludelist'] == 0) ]\n",
    "    \n",
    "print('Count of filtered participants:', filtered_data.shape[0])\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpos = sorted(filtered_data.hpo.unique())\n",
    "site_names = pd.read_csv('../data/sites.csv')\n",
    "sorted_hpos_ids = site_names.hpo_id.unique()\n",
    "\n",
    "\n",
    "######################################\n",
    "print('Generating PEO Data...')\n",
    "######################################\n",
    "print()\n",
    "\n",
    "## Heading\n",
    "print(' ', sep='', end='')\n",
    "for hpo_id in sorted_hpos_ids:\n",
    "    short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "    hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    print(',', short_name, sep='', end='')\n",
    "print()\n",
    "\n",
    "    \n",
    "### All Participants ###\n",
    "print('All', sep='', end='')\n",
    "for hpo_id in sorted_hpos_ids:\n",
    "    short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "    hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    \n",
    "    all_Ppts_hpo = filtered_data[\n",
    "        (filtered_data['hpo'] == hpo) &\n",
    "        (filtered_data['registered_date'] <= d) \n",
    "        ].shape[0]\n",
    "    all_Ppts_prev_hpo = filtered_data[\n",
    "        (filtered_data['hpo'] == hpo) &\n",
    "        (filtered_data['registered_date'] <= d_prev) \n",
    "        ].shape[0]\n",
    "    \n",
    "    print(',', all_Ppts_hpo, sep='', end='')\n",
    "print()\n",
    "\n",
    "### Since last report ###\n",
    "print('24h', sep='', end='')\n",
    "for hpo_id in sorted_hpos_ids:\n",
    "    short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "    hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    \n",
    "    all_Ppts_hpo = filtered_data[\n",
    "        (filtered_data['hpo'] == hpo) &\n",
    "        (filtered_data['registered_date'] <= d) \n",
    "        ].shape[0]\n",
    "    all_Ppts_prev_hpo = filtered_data[\n",
    "        (filtered_data['hpo'] == hpo) &\n",
    "        (filtered_data['registered_date'] <= d_prev) \n",
    "        ].shape[0]\n",
    "    \n",
    "    print(',', all_Ppts_hpo - all_Ppts_prev_hpo, sep='', end='')\n",
    "print()\n",
    "\n",
    "### Core Participants ###\n",
    "print('Core', sep='', end='')\n",
    "for hpo_id in sorted_hpos_ids:\n",
    "    short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "    hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    \n",
    "    core_Ppts_hpo = filtered_data[\n",
    "        (filtered_data['hpo'] == hpo) &\n",
    "        (np.isnat(filtered_data['fp_date']) == False) &\n",
    "        (filtered_data['fp_date'] <= d) \n",
    "        ].shape[0]\n",
    "    \n",
    "    print(',', core_Ppts_hpo, sep='', end='')\n",
    "print()\n",
    "\n",
    "### Members ###\n",
    "print('Members', sep='', end='')\n",
    "for hpo_id in sorted_hpos_ids:\n",
    "    short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "    hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    \n",
    "    member_Ppts_hpo = filtered_data[\n",
    "        (filtered_data['hpo'] == hpo) &\n",
    "        (np.isnat(filtered_data['member_date']) == False) &\n",
    "        (filtered_data['member_date'] <= d) &\n",
    "        ((np.isnat(filtered_data['fp_date']) | (filtered_data['fp_date'] > d))) \n",
    "        ].shape[0]\n",
    "    \n",
    "    print(',', member_Ppts_hpo, sep='', end='')\n",
    "print()\n",
    "\n",
    "### Registered ###\n",
    "print('Registered', sep='', end='')\n",
    "for hpo_id in sorted_hpos_ids:\n",
    "    short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "    hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    \n",
    "    registered_Ppts_hpo = filtered_data[\n",
    "        (filtered_data['hpo'] == hpo) &\n",
    "        (filtered_data['registered_date'] <= d) &\n",
    "        ((np.isnat(filtered_data['member_date'])) | (filtered_data['member_date'] > d))\n",
    "        ].shape[0]\n",
    "    \n",
    "    print(',', registered_Ppts_hpo, sep='', end='')\n",
    "print()\n",
    "\n",
    "### Gender ID ###\n",
    "\n",
    "genderIDs = pd.read_csv('../data/gender.csv')\n",
    "sorted_genderIDs = genderIDs.gender_id.unique()\n",
    "\n",
    "#Iterate through gender IDs and then within each, iterate through sites        \n",
    "for genderID in sorted_genderIDs:\n",
    "    g_id = genderIDs[genderIDs['gender_id'].str.match(genderID)].values.flatten().tolist()[0]\n",
    "    label = genderIDs[genderIDs['gender_id'].str.match(genderID)].values.flatten().tolist()[1]\n",
    "    print(label, end='')\n",
    "    for hpo_id in sorted_hpos_ids:\n",
    "        short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "        hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "        \n",
    "        #Gender Identity\n",
    "        if genderID == 'None':\n",
    "            all_Ppts_gender = filtered_data[\n",
    "                (filtered_data['hpo'] == hpo) &\n",
    "                pd.isnull(filtered_data['gender']) &\n",
    "                (filtered_data['registered_date'] <= d)\n",
    "                ].shape[0]\n",
    "            print(',', all_Ppts_gender, sep='', end='')\n",
    "        else:    \n",
    "            all_Ppts_gender = filtered_data[\n",
    "                (filtered_data['hpo'] == hpo) &\n",
    "                (filtered_data['gender'] == g_id) &\n",
    "                (filtered_data['registered_date'] <= d)\n",
    "                ].shape[0]\n",
    "            print(',', all_Ppts_gender, sep='', end = '')\n",
    "    print()\n",
    "    \n",
    "### Race ###\n",
    "\n",
    "races = pd.read_csv('../data/race.csv')\n",
    "sorted_races = races.race.unique()\n",
    "\n",
    "#Iterate through races and then within each, iterate through sites        \n",
    "for race in sorted_races:\n",
    "    race_id = races.loc[races['race'] == race].values.flatten().tolist()[0]\n",
    "    label = races.loc[races['race'] == race].values.flatten().tolist()[2]\n",
    "    print(label, end='')\n",
    "    for hpo_id in sorted_hpos_ids:\n",
    "        short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "        hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    \n",
    "        if race == 'None':\n",
    "            all_Ppts_race = filtered_data[\n",
    "                (filtered_data['hpo'] == hpo) &\n",
    "                pd.isnull(filtered_data['race']) &\n",
    "                (filtered_data['registered_date'] <= d)\n",
    "                ].shape[0]\n",
    "            print(',', all_Ppts_race, sep='', end='')\n",
    "        else:    \n",
    "            all_Ppts_race = filtered_data[\n",
    "                (filtered_data['hpo'] == hpo) &\n",
    "                (filtered_data['race'] == race_id) &\n",
    "                (filtered_data['registered_date'] <= d)\n",
    "                ].shape[0]\n",
    "            print(',', all_Ppts_race, sep='', end = '')\n",
    "    print()\n",
    "\n",
    "### Age Buckets ###\n",
    "\n",
    "ages = pd.read_csv('../data/age.csv')\n",
    "sorted_ages = ages.age.unique()\n",
    "        \n",
    "#Iterate through ages and then within each, iterate through sites        \n",
    "for age in sorted_ages:\n",
    "    age_id = ages[ages['age'].str.match(age)].values.flatten().tolist()[0]\n",
    "    label = ages[ages['age'].str.match(age)].values.flatten().tolist()[1]\n",
    "    print(label, end='')\n",
    "    for hpo_id in sorted_hpos_ids:\n",
    "        short_name = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[3]\n",
    "        hpo = site_names[site_names['hpo_id'].str.match(hpo_id)].values.flatten().tolist()[1]\n",
    "    \n",
    "        if age == 'None':\n",
    "            all_Ppts_age = filtered_data[\n",
    "                (filtered_data['hpo'] == hpo) &\n",
    "                pd.isnull(filtered_data['age_bucket']) &\n",
    "                (filtered_data['registered_date'] <= d)\n",
    "                ].shape[0]\n",
    "            print(',', all_Ppts_age, sep='', end='')\n",
    "        else:    \n",
    "            all_Ppts_age = filtered_data[\n",
    "                (filtered_data['hpo'] == hpo) &\n",
    "                (filtered_data['age_bucket'] == age_id) &\n",
    "                (filtered_data['registered_date'] <= d)\n",
    "                ].shape[0]\n",
    "            print(',', all_Ppts_age, sep='', end = '')\n",
    "    print()\n",
    "print()\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "print('done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
